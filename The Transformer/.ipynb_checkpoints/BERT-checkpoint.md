# BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding

#### Notes by: Katie Haller

![BERT architecture](./images/BERT-Fig.1.png)
<sup>[1](#myfootnote1)</sup> 




Resources:

<a name="myfootnote1">1</a>: [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)


